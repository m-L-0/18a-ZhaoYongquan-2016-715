{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 下载MNIST数据集并生成DataSet对象\n",
    "# 使用OneHot编码处理标记\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.pool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集图片矩阵，代表55000张图片，每张图片为一个向量，其长度为784\n",
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集标记矩阵，代表55000张图片的标记，每张图片为一个10维的独热编码向量\n",
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20220cb5b70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化训练集中的图片\n",
    "plt.imshow(Image.fromarray(mnist.train.images[0].reshape(28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\pydp\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\pydp\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "step     0, loss 13.7659, acc 0.1135\n",
      "step   500, loss 11.8558, acc 0.3320\n",
      "step  1000, loss 6.3970, acc 0.4407\n",
      "step  1500, loss 7.0626, acc 0.4876\n",
      "step  2000, loss 9.3546, acc 0.5096\n",
      "step  2500, loss 7.4074, acc 0.5275\n",
      "step  3000, loss 6.5146, acc 0.5370\n",
      "step  3500, loss 7.5560, acc 0.5429\n",
      "step  4000, loss 7.0516, acc 0.5508\n",
      "step  4500, loss 6.7008, acc 0.5629\n",
      "step  5000, loss 5.0369, acc 0.5763\n",
      "step  5500, loss 6.9568, acc 0.5952\n",
      "step  6000, loss 4.5335, acc 0.6130\n",
      "step  6500, loss 2.4429, acc 0.6380\n",
      "step  7000, loss 6.1986, acc 0.6854\n",
      "step  7500, loss 2.5222, acc 0.7048\n",
      "step  8000, loss 4.0955, acc 0.7221\n",
      "step  8500, loss 5.1954, acc 0.7396\n",
      "step  9000, loss 3.5259, acc 0.7586\n",
      "step  9500, loss 2.4457, acc 0.7700\n",
      "step 10000, loss 3.6616, acc 0.7765\n",
      "step 10500, loss 2.0608, acc 0.7823\n",
      "step 11000, loss 3.7274, acc 0.7878\n",
      "step 11500, loss 2.7683, acc 0.7896\n",
      "step 12000, loss 3.2087, acc 0.7908\n",
      "step 12500, loss 4.7642, acc 0.7990\n",
      "step 13000, loss 3.0698, acc 0.7992\n",
      "step 13500, loss 3.0156, acc 0.8034\n",
      "step 14000, loss 2.9974, acc 0.8015\n",
      "step 14500, loss 2.0149, acc 0.8043\n",
      "step 15000, loss 2.7248, acc 0.8056\n",
      "step 15500, loss 2.5392, acc 0.8093\n",
      "step 16000, loss 2.5185, acc 0.8060\n",
      "step 16500, loss 2.0153, acc 0.8105\n",
      "step 17000, loss 4.1997, acc 0.8096\n",
      "step 17500, loss 2.3663, acc 0.8091\n",
      "step 18000, loss 1.5473, acc 0.8127\n",
      "step 18500, loss 3.0221, acc 0.8124\n",
      "step 19000, loss 2.5185, acc 0.8126\n",
      "step 19500, loss 2.3212, acc 0.8135\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "1. 浮现上述代码。\n",
    "2. 计算模型参数。\n",
    "3. 使用不同大小的批次进行实验，观察模型收敛速度与收敛平稳性。\n",
    "4. 使用不同大小的学习率进行实验，观察模型收敛速度与收敛平稳性。\n",
    "5. 使用不同的激活函数进行实验，观察模型收敛速度与收敛平性能。\n",
    "6. 尝试使用不同的参数初始化方法（如使用不同的正态分布、均匀分布、固定值等方法），观察模型收敛速度。\n",
    "7. 思考如何改进模型以使得模型性能增强。\n",
    "8. 思考如何给模型添加新的隐藏层并进行实验。\n",
    "\n",
    "*说明：需对上述问题进行代码实现与结论总结*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 其他条件保持不变，分别将原来批次修改成64和16，发现64的正确率相比32的有所下降，而16的相比32有所上升；当批次大小变大时，模型的收敛速度会加快，但是训练误差不稳步下降；当批次大小变小时，发现模型收敛速度变慢，当误差下降的相对更稳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 14.3527, acc 0.0889\n",
      "step   500, loss 12.1756, acc 0.2902\n",
      "step  1000, loss 7.7367, acc 0.4304\n",
      "step  1500, loss 7.3379, acc 0.4987\n",
      "step  2000, loss 5.8388, acc 0.5317\n",
      "step  2500, loss 6.1692, acc 0.5593\n",
      "step  3000, loss 7.8774, acc 0.5933\n",
      "step  3500, loss 6.4740, acc 0.6167\n",
      "step  4000, loss 5.0948, acc 0.6406\n",
      "step  4500, loss 4.5671, acc 0.6491\n",
      "step  5000, loss 3.7844, acc 0.6597\n",
      "step  5500, loss 4.6567, acc 0.6605\n",
      "step  6000, loss 4.9594, acc 0.6669\n",
      "step  6500, loss 5.0375, acc 0.6783\n",
      "step  7000, loss 4.8272, acc 0.6766\n",
      "step  7500, loss 4.2814, acc 0.6795\n",
      "step  8000, loss 5.4769, acc 0.6819\n",
      "step  8500, loss 4.5784, acc 0.6870\n",
      "step  9000, loss 5.7302, acc 0.6891\n",
      "step  9500, loss 5.7463, acc 0.6913\n",
      "step 10000, loss 5.6437, acc 0.6898\n",
      "step 10500, loss 4.2814, acc 0.6969\n",
      "step 11000, loss 5.0225, acc 0.6989\n",
      "step 11500, loss 4.1043, acc 0.6954\n",
      "step 12000, loss 4.3970, acc 0.6963\n",
      "step 12500, loss 3.6254, acc 0.7057\n",
      "step 13000, loss 5.0411, acc 0.7011\n",
      "step 13500, loss 3.4403, acc 0.7007\n",
      "step 14000, loss 5.3405, acc 0.7055\n",
      "step 14500, loss 6.4212, acc 0.7044\n",
      "step 15000, loss 4.7552, acc 0.7072\n",
      "step 15500, loss 4.8493, acc 0.7088\n",
      "step 16000, loss 2.7816, acc 0.7052\n",
      "step 16500, loss 5.5772, acc 0.7096\n",
      "step 17000, loss 3.5215, acc 0.7065\n",
      "step 17500, loss 3.4235, acc 0.7149\n",
      "step 18000, loss 3.7854, acc 0.7110\n",
      "step 18500, loss 3.5134, acc 0.7122\n",
      "step 19000, loss 3.6272, acc 0.7075\n",
      "step 19500, loss 3.6244, acc 0.7088\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(64)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 13.7379, acc 0.0839\n",
      "step   500, loss 9.2593, acc 0.3700\n",
      "step  1000, loss 7.0164, acc 0.4986\n",
      "step  1500, loss 6.9927, acc 0.5773\n",
      "step  2000, loss 5.6142, acc 0.6292\n",
      "step  2500, loss 6.0180, acc 0.6540\n",
      "step  3000, loss 4.5560, acc 0.6925\n",
      "step  3500, loss 4.1323, acc 0.7200\n",
      "step  4000, loss 2.8417, acc 0.7286\n",
      "step  4500, loss 3.5667, acc 0.7506\n",
      "step  5000, loss 3.3225, acc 0.7530\n",
      "step  5500, loss 3.6103, acc 0.7674\n",
      "step  6000, loss 4.1849, acc 0.7712\n",
      "step  6500, loss 4.0601, acc 0.7744\n",
      "step  7000, loss 3.1643, acc 0.7837\n",
      "step  7500, loss 3.3448, acc 0.7778\n",
      "step  8000, loss 2.4979, acc 0.7891\n",
      "step  8500, loss 1.5283, acc 0.8273\n",
      "step  9000, loss 1.8288, acc 0.8496\n",
      "step  9500, loss 1.8989, acc 0.8578\n",
      "step 10000, loss 2.1294, acc 0.8718\n",
      "step 10500, loss 0.5762, acc 0.8662\n",
      "step 11000, loss 1.6304, acc 0.8770\n",
      "step 11500, loss 1.0390, acc 0.8826\n",
      "step 12000, loss 1.3489, acc 0.8830\n",
      "step 12500, loss 1.3941, acc 0.8788\n",
      "step 13000, loss 1.5038, acc 0.8882\n",
      "step 13500, loss 1.4045, acc 0.8856\n",
      "step 14000, loss 1.3382, acc 0.8954\n",
      "step 14500, loss 1.3471, acc 0.8882\n",
      "step 15000, loss 1.4160, acc 0.8954\n",
      "step 15500, loss 1.4041, acc 0.8952\n",
      "step 16000, loss 1.2870, acc 0.8902\n",
      "step 16500, loss 1.2334, acc 0.9006\n",
      "step 17000, loss 1.2125, acc 0.8970\n",
      "step 17500, loss 1.7103, acc 0.9042\n",
      "step 18000, loss 1.2379, acc 0.8932\n",
      "step 18500, loss 1.4267, acc 0.9071\n",
      "step 19000, loss 1.4506, acc 0.8990\n",
      "step 19500, loss 1.2751, acc 0.9058\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(128)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(16)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用不同大小的学习率进行实验，观察模型收敛速度与收敛平稳性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 13.2972, acc 0.0999\n",
      "step   500, loss 4.8398, acc 0.6902\n",
      "step  1000, loss 3.9271, acc 0.7210\n",
      "step  1500, loss 3.9644, acc 0.7413\n",
      "step  2000, loss 4.0281, acc 0.7435\n",
      "step  2500, loss 2.9047, acc 0.7485\n",
      "step  3000, loss 3.1895, acc 0.7493\n",
      "step  3500, loss 2.5322, acc 0.7545\n",
      "step  4000, loss 3.8324, acc 0.7547\n",
      "step  4500, loss 4.5552, acc 0.7643\n",
      "step  5000, loss 2.2440, acc 0.8196\n",
      "step  5500, loss 2.9556, acc 0.8223\n",
      "step  6000, loss 2.7184, acc 0.8348\n",
      "step  6500, loss 2.4556, acc 0.8332\n",
      "step  7000, loss 2.8535, acc 0.8408\n",
      "step  7500, loss 2.0822, acc 0.8346\n",
      "step  8000, loss 2.8166, acc 0.8361\n",
      "step  8500, loss 2.0322, acc 0.8450\n",
      "step  9000, loss 2.1688, acc 0.8415\n",
      "step  9500, loss 1.5450, acc 0.8439\n",
      "step 10000, loss 1.6838, acc 0.8469\n",
      "step 10500, loss 2.1554, acc 0.8468\n",
      "step 11000, loss 2.1638, acc 0.8481\n",
      "step 11500, loss 2.2678, acc 0.8491\n",
      "step 12000, loss 2.6627, acc 0.8452\n",
      "step 12500, loss 1.7637, acc 0.8490\n",
      "step 13000, loss 2.5695, acc 0.8513\n",
      "step 13500, loss 1.6596, acc 0.8495\n",
      "step 14000, loss 2.3927, acc 0.8473\n",
      "step 14500, loss 1.3906, acc 0.8519\n",
      "step 15000, loss 2.5234, acc 0.8535\n",
      "step 15500, loss 1.8903, acc 0.8498\n",
      "step 16000, loss 1.6534, acc 0.8499\n",
      "step 16500, loss 1.8718, acc 0.8536\n",
      "step 17000, loss 1.7671, acc 0.8485\n",
      "step 17500, loss 2.3928, acc 0.8569\n",
      "step 18000, loss 2.6445, acc 0.8474\n",
      "step 18500, loss 1.7659, acc 0.8509\n",
      "step 19000, loss 1.6391, acc 0.8499\n",
      "step 19500, loss 2.7709, acc 0.8583\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(128)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 12.2607, acc 0.1536\n",
      "step   500, loss 12.6064, acc 0.1875\n",
      "step  1000, loss 12.5209, acc 0.2043\n",
      "step  1500, loss 12.3345, acc 0.2244\n",
      "step  2000, loss 12.4778, acc 0.2412\n",
      "step  2500, loss 12.2389, acc 0.2470\n",
      "step  3000, loss 12.0452, acc 0.2766\n",
      "step  3500, loss 11.6201, acc 0.2756\n",
      "step  4000, loss 10.0868, acc 0.2973\n",
      "step  4500, loss 10.6252, acc 0.3049\n",
      "step  5000, loss 10.3079, acc 0.3109\n",
      "step  5500, loss 11.1483, acc 0.3277\n",
      "step  6000, loss 10.6887, acc 0.3209\n",
      "step  6500, loss 9.6206, acc 0.3438\n",
      "step  7000, loss 9.7970, acc 0.3506\n",
      "step  7500, loss 10.8906, acc 0.3512\n",
      "step  8000, loss 10.0424, acc 0.3566\n",
      "step  8500, loss 10.1256, acc 0.3678\n",
      "step  9000, loss 9.9283, acc 0.3726\n",
      "step  9500, loss 8.8321, acc 0.3788\n",
      "step 10000, loss 10.5412, acc 0.3862\n",
      "step 10500, loss 9.5811, acc 0.3992\n",
      "step 11000, loss 8.7624, acc 0.3862\n",
      "step 11500, loss 8.6453, acc 0.3964\n",
      "step 12000, loss 9.1062, acc 0.4087\n",
      "step 12500, loss 9.1875, acc 0.4109\n",
      "step 13000, loss 9.5922, acc 0.4179\n",
      "step 13500, loss 8.6243, acc 0.4119\n",
      "step 14000, loss 9.2224, acc 0.4257\n",
      "step 14500, loss 8.7083, acc 0.4317\n",
      "step 15000, loss 9.1059, acc 0.4207\n",
      "step 15500, loss 8.5204, acc 0.4337\n",
      "step 16000, loss 9.4631, acc 0.4341\n",
      "step 16500, loss 9.1723, acc 0.4435\n",
      "step 17000, loss 9.8393, acc 0.4369\n",
      "step 17500, loss 8.5747, acc 0.4397\n",
      "step 18000, loss 10.5306, acc 0.4503\n",
      "step 18500, loss 7.9133, acc 0.4443\n",
      "step 19000, loss 9.1186, acc 0.4501\n",
      "step 19500, loss 8.9198, acc 0.4459\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(128)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(16)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的学习率分别为0.1和0.001，其他条件相同。观察训练结果发现，学习率调高，会使训练速度加快，但不稳；调小学习率，会使训练变慢，模型平稳性更好；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 使用不同的激活函数进行实验，观察模型收敛速度与收敛性能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 9.4845, acc 0.1310\n",
      "step   500, loss -4.2885, acc 0.2039\n",
      "step  1000, loss -4.9472, acc 0.2157\n",
      "step  1500, loss -5.1340, acc 0.2246\n",
      "step  2000, loss -5.2431, acc 0.2470\n",
      "step  2500, loss -5.3106, acc 0.2520\n",
      "step  3000, loss -5.4150, acc 0.2648\n",
      "step  3500, loss -5.5696, acc 0.2708\n",
      "step  4000, loss -5.6074, acc 0.2827\n",
      "step  4500, loss -5.7004, acc 0.2951\n",
      "step  5000, loss -5.6994, acc 0.2875\n",
      "step  5500, loss -5.8147, acc 0.3071\n",
      "step  6000, loss -5.8488, acc 0.3037\n",
      "step  6500, loss -5.8320, acc 0.3153\n",
      "step  7000, loss -5.9933, acc 0.3283\n",
      "step  7500, loss -6.0026, acc 0.3257\n",
      "step  8000, loss -6.0594, acc 0.3235\n",
      "step  8500, loss -6.0867, acc 0.3317\n",
      "step  9000, loss -6.1159, acc 0.3538\n",
      "step  9500, loss -6.1771, acc 0.3397\n",
      "step 10000, loss -6.2499, acc 0.3401\n",
      "step 10500, loss -6.2439, acc 0.3540\n",
      "step 11000, loss -6.2326, acc 0.3578\n",
      "step 11500, loss -6.2843, acc 0.3600\n",
      "step 12000, loss -6.2994, acc 0.3572\n",
      "step 12500, loss -6.3838, acc 0.3708\n",
      "step 13000, loss -6.3445, acc 0.3672\n",
      "step 13500, loss -6.4254, acc 0.3674\n",
      "step 14000, loss -6.4128, acc 0.3792\n",
      "step 14500, loss -6.4522, acc 0.3740\n",
      "step 15000, loss -6.5449, acc 0.3826\n",
      "step 15500, loss -6.5599, acc 0.3822\n",
      "step 16000, loss -6.5215, acc 0.3810\n",
      "step 16500, loss -6.6137, acc 0.3792\n",
      "step 17000, loss -6.6599, acc 0.3936\n",
      "step 17500, loss -6.6198, acc 0.3876\n",
      "step 18000, loss -6.6721, acc 0.3934\n",
      "step 18500, loss -6.7227, acc 0.4044\n",
      "step 19000, loss -6.6933, acc 0.4006\n",
      "step 19500, loss -6.7317, acc 0.4000\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "#     output = tf.nn.softmax(logits)\n",
    "    output = tf.nn.relu(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(64)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(16)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 尝试使用不同的参数初始化方法（如使用不同的正态分布、均匀分布、固定值等方法），观察模型收敛速度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 14.3751, acc 0.0549\n",
      "step   500, loss 14.4842, acc 0.0681\n",
      "step  1000, loss 15.0892, acc 0.0913\n",
      "step  1500, loss 13.6254, acc 0.1168\n",
      "step  2000, loss 13.7814, acc 0.1316\n",
      "step  2500, loss 12.8922, acc 0.1548\n",
      "step  3000, loss 13.9217, acc 0.1711\n",
      "step  3500, loss 13.5558, acc 0.1853\n",
      "step  4000, loss 12.4827, acc 0.1941\n",
      "step  4500, loss 11.9097, acc 0.2157\n",
      "step  5000, loss 11.6202, acc 0.2145\n",
      "step  5500, loss 11.1616, acc 0.2310\n",
      "step  6000, loss 11.7236, acc 0.2438\n",
      "step  6500, loss 12.3821, acc 0.2458\n",
      "step  7000, loss 12.0161, acc 0.2542\n",
      "step  7500, loss 10.7254, acc 0.2722\n",
      "step  8000, loss 11.2353, acc 0.2662\n",
      "step  8500, loss 11.6090, acc 0.2734\n",
      "step  9000, loss 11.7761, acc 0.2762\n",
      "step  9500, loss 11.4290, acc 0.2786\n",
      "step 10000, loss 12.1089, acc 0.2917\n",
      "step 10500, loss 10.9486, acc 0.2953\n",
      "step 11000, loss 12.3234, acc 0.2887\n",
      "step 11500, loss 12.4901, acc 0.2965\n",
      "step 12000, loss 10.1717, acc 0.2913\n",
      "step 12500, loss 11.8163, acc 0.2983\n",
      "step 13000, loss 10.6986, acc 0.3009\n",
      "step 13500, loss 10.8623, acc 0.3021\n",
      "step 14000, loss 11.7755, acc 0.3075\n",
      "step 14500, loss 10.6819, acc 0.3003\n",
      "step 15000, loss 10.5292, acc 0.3153\n",
      "step 15500, loss 10.6824, acc 0.3049\n",
      "step 16000, loss 11.2805, acc 0.3107\n",
      "step 16500, loss 10.7971, acc 0.3159\n",
      "step 17000, loss 10.4360, acc 0.3101\n",
      "step 17500, loss 12.0633, acc 0.3163\n",
      "step 18000, loss 10.1687, acc 0.3103\n",
      "step 18500, loss 11.1726, acc 0.3147\n",
      "step 19000, loss 11.5648, acc 0.3219\n",
      "step 19500, loss 10.7062, acc 0.3115\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    \n",
    "    hidden_weight = tf.Variable(tf.truncated_normal([784, 128]), name='hidden_weight')\n",
    "#     hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(128)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(16)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 思考如何改进模型以使得模型性能增强。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以调整模型的学习率、训练批次的大小、激活函数、增加隐藏层、增加训练总的步数等来查看是否会提升模型性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 思考如何给模型添加新的隐藏层并进行实验\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26037482550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Image.fromarray(mnist.train.images[9].reshape(28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 15.6144, acc 0.0663\n",
      "step   500, loss 13.5996, acc 0.1597\n",
      "step  1000, loss 13.0960, acc 0.1715\n",
      "step  1500, loss 13.0960, acc 0.1920\n",
      "step  2000, loss 12.0886, acc 0.2304\n",
      "step  2500, loss 13.5996, acc 0.2517\n",
      "step  3000, loss 12.5923, acc 0.2633\n",
      "step  3500, loss 11.0812, acc 0.2703\n",
      "step  4000, loss 10.5775, acc 0.2722\n",
      "step  4500, loss 12.0886, acc 0.2746\n",
      "step  5000, loss 10.0738, acc 0.2788\n",
      "step  5500, loss 11.5849, acc 0.2729\n",
      "step  6000, loss 10.5775, acc 0.2856\n",
      "step  6500, loss 12.0886, acc 0.2781\n",
      "step  7000, loss 10.5775, acc 0.2826\n",
      "step  7500, loss 11.5849, acc 0.2817\n",
      "step  8000, loss 12.5923, acc 0.2844\n",
      "step  8500, loss 11.5849, acc 0.2833\n",
      "step  9000, loss 10.5775, acc 0.3847\n",
      "step  9500, loss 7.5554, acc 0.4399\n",
      "step 10000, loss 9.2250, acc 0.4571\n",
      "step 10500, loss 8.1089, acc 0.4779\n",
      "step 11000, loss 6.5480, acc 0.4973\n",
      "step 11500, loss 6.5480, acc 0.5356\n",
      "step 12000, loss 8.0590, acc 0.5495\n",
      "step 12500, loss 6.5480, acc 0.5511\n",
      "step 13000, loss 4.3386, acc 0.5689\n",
      "step 13500, loss 10.5776, acc 0.5575\n",
      "step 14000, loss 7.5554, acc 0.5608\n",
      "step 14500, loss 7.0517, acc 0.5613\n",
      "step 15000, loss 5.5406, acc 0.5565\n",
      "step 15500, loss 6.5480, acc 0.5659\n",
      "step 16000, loss 7.5554, acc 0.5657\n",
      "step 16500, loss 7.0517, acc 0.5686\n",
      "step 17000, loss 7.5554, acc 0.5687\n",
      "step 17500, loss 5.5406, acc 0.5662\n",
      "step 18000, loss 3.5258, acc 0.5676\n",
      "step 18500, loss 6.0443, acc 0.5699\n",
      "step 19000, loss 8.5627, acc 0.5735\n",
      "step 19500, loss 7.0517, acc 0.5916\n",
      "step 20000, loss 9.1437, acc 0.6056\n",
      "step 20500, loss 6.5480, acc 0.6158\n",
      "step 21000, loss 6.1213, acc 0.6345\n",
      "step 21500, loss 7.5554, acc 0.6375\n",
      "step 22000, loss 7.5554, acc 0.6393\n",
      "step 22500, loss 8.0590, acc 0.6511\n",
      "step 23000, loss 9.3586, acc 0.6578\n",
      "step 23500, loss 3.0221, acc 0.6515\n",
      "step 24000, loss 5.0369, acc 0.6654\n",
      "step 24500, loss 3.5258, acc 0.6628\n",
      "step 25000, loss 4.0251, acc 0.6722\n",
      "step 25500, loss 5.0369, acc 0.6730\n",
      "step 26000, loss 4.0295, acc 0.6973\n",
      "step 26500, loss 3.5258, acc 0.7260\n",
      "step 27000, loss 4.5332, acc 0.7237\n",
      "step 27500, loss 3.5258, acc 0.7592\n",
      "step 28000, loss 3.0221, acc 0.7789\n",
      "step 28500, loss 2.0148, acc 0.7799\n",
      "step 29000, loss 3.5258, acc 0.7954\n",
      "step 29500, loss 3.0221, acc 0.7799\n",
      "step 30000, loss 3.5258, acc 0.7948\n",
      "step 30500, loss 3.0221, acc 0.7965\n",
      "step 31000, loss 3.0221, acc 0.7933\n",
      "step 31500, loss 3.0063, acc 0.8021\n",
      "step 32000, loss 2.5188, acc 0.8075\n",
      "step 32500, loss 3.5258, acc 0.8034\n",
      "step 33000, loss 5.0370, acc 0.8174\n",
      "step 33500, loss 2.5185, acc 0.8126\n",
      "step 34000, loss 2.0148, acc 0.8143\n",
      "step 34500, loss 2.5185, acc 0.8084\n",
      "step 35000, loss 4.0295, acc 0.8049\n",
      "step 35500, loss 2.5185, acc 0.8196\n",
      "step 36000, loss 3.0221, acc 0.8195\n",
      "step 36500, loss 2.8847, acc 0.8191\n",
      "step 37000, loss 3.5258, acc 0.8205\n",
      "step 37500, loss 2.5185, acc 0.8140\n",
      "step 38000, loss 2.5185, acc 0.8208\n",
      "step 38500, loss 1.5111, acc 0.8230\n",
      "step 39000, loss 4.0295, acc 0.8265\n",
      "step 39500, loss 2.0148, acc 0.8257\n",
      "step 40000, loss 4.0295, acc 0.8155\n",
      "step 40500, loss 1.5111, acc 0.8299\n",
      "step 41000, loss 1.4751, acc 0.8246\n",
      "step 41500, loss 3.0221, acc 0.8285\n",
      "step 42000, loss 3.0221, acc 0.8285\n",
      "step 42500, loss 3.5258, acc 0.8273\n",
      "step 43000, loss 1.5111, acc 0.8226\n",
      "step 43500, loss 2.5513, acc 0.8178\n",
      "step 44000, loss 2.0148, acc 0.8299\n",
      "step 44500, loss 3.5089, acc 0.8257\n",
      "step 45000, loss 3.5258, acc 0.8323\n",
      "step 45500, loss 4.0295, acc 0.8314\n",
      "step 46000, loss 2.5185, acc 0.8226\n",
      "step 46500, loss 3.4563, acc 0.8296\n",
      "step 47000, loss 4.0295, acc 0.8288\n",
      "step 47500, loss 1.0074, acc 0.8326\n",
      "step 48000, loss 2.0148, acc 0.8330\n",
      "step 48500, loss 3.5258, acc 0.8337\n",
      "step 49000, loss 3.0221, acc 0.8361\n",
      "step 49500, loss 1.5111, acc 0.8312\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    h1_w = tf.Variable(tf.random_normal([784, 64]), name='h1_w')\n",
    "    h1_b = tf.Variable(tf.zeros([64, ]), name='h1_b')\n",
    "    h1_o = tf.nn.relu(tf.matmul(inputs, h1_w)+h1_b)\n",
    "    \n",
    "    h2_w = tf.Variable(tf.random_normal([64, 128]), name='h2_w')\n",
    "    h2_b = tf.Variable(tf.zeros([128, ]), name='h2_b')\n",
    "    h2_o = tf.nn.relu(tf.matmul(h1_o, h2_w)+h2_b)\n",
    "    \n",
    "    o_w = tf.Variable(tf.random_normal([128, 10]), name='o_w')\n",
    "    o_b = tf.Variable(tf.zeros([10, ]), name='o_w')\n",
    "    \n",
    "    logits = tf.matmul(h2_o, o_w) + o_b\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "            labels * tf.log(output + 1e-7), axis=1))\n",
    "    \n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)), \n",
    "                dtype=tf.float32))\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        \n",
    "        train_op = optim.minimize(loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(50000):\n",
    "            batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "            res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "                        inputs: batch_images,\n",
    "                        labels: batch_labels\n",
    "            })\n",
    "            \n",
    "            if step % 500 ==0:\n",
    "                accs = []\n",
    "                for test_step in range(10000 // 32):\n",
    "                    batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                    \n",
    "                    res_acc = sess.run(acc, feed_dict={\n",
    "                        inputs: batch_images,\n",
    "                        labels: batch_labels\n",
    "                    })\n",
    "                    \n",
    "                    accs.append(res_acc)\n",
    "                accs = np.mean(accs)\n",
    "                print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydp-notebook",
   "language": "python",
   "name": "pydp-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
